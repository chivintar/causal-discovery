{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e391cd6b-50d9-4a1c-94e0-6b4bd6f49812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# #!pip install ipympl\n",
    "# %matplotlib widget\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../external\"))\n",
    "sys.path.append(os.path.abspath(\"../external/causal_comp\"))\n",
    "sys.path.append(os.path.abspath(\"../external/tigramite\"))\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbaed5-3e0e-488a-a106-216b86446ca2",
   "metadata": {},
   "source": [
    "# 2D model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc60eab-be13-424a-ade3-e769f8c759ad",
   "metadata": {},
   "source": [
    "## LKIFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d31e2a3-af84-4608-81be-64d73ca4258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_comp.function_liang_nvar import compute_liang_nvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53026c90-fb66-4c4c-9827-47b46286142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_comp.function_liang_nvar import compute_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "207076b7-f145-4091-8d68-dbfb4202bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series\n",
    "load_series = True # True: load time series; False: compute and save time series\n",
    "save_var = True # True: save Liang index and correlation coefficient (for plotting afterwards); False: don't save variables\n",
    "\n",
    "filename = '../data/2D_series.npy'\n",
    "var_filename = '../data/2D_liang.npy'\n",
    "if load_series == True: # load time series\n",
    "    t, X1, X2 = np.load(filename, allow_pickle=True)\n",
    "    T, tau, R, error_T, error_tau, error_R, sig_T, sig_tau, sig_R = np.load(var_filename, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d23cdc5-bac7-457c-8479-56e81e384aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Parameters and Simulation Settings\n",
    "dt = 0.001\n",
    "tmax = 1000 # takes around 9 mins\n",
    "nt = int(tmax / dt)\n",
    "t = np.linspace(0, tmax, nt)\n",
    "start_computation = int(10 / dt)  # discard transients\n",
    "n_iter = 200 # bootstrap samples for Liang\n",
    "nvar = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2c438f-5cc1-4b01-8985-f83b2fbc5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_series==False:\n",
    "    # Simulate 2D Time Series\n",
    "    a11, a12 = -1, 0.5\n",
    "    a21, a22 = 0, -1\n",
    "    sigma1, sigma2 = 0.1, 0.1\n",
    "    dW1 = np.sqrt(dt) * np.random.normal(0, 1, nt)\n",
    "    dW2 = np.sqrt(dt) * np.random.normal(0, 1, nt)\n",
    "    X1, X2 = np.zeros(nt), np.zeros(nt)\n",
    "    X1[0], X2[0] = 1.0, 2.0\n",
    "    \n",
    "    for i in range(nt - 1):\n",
    "        X1[i + 1] = X1[i] + (a11 * X1[i] + a12 * X2[i]) * dt + sigma1 * dW1[i]\n",
    "        X2[i + 1] = X2[i] + (a22 * X2[i] + a21 * X1[i]) * dt + sigma2 * dW2[i]\n",
    "    \n",
    "    # Apply Liang-Kleeman Info Flow on Post-Transient Segment\n",
    "    print(\"Computing the Information Flow Rate...\")\n",
    "    start = time.time()\n",
    "    xx = np.array([X1[start_computation:], X2[start_computation:]])\n",
    "    T, tau, R, error_T, error_tau, error_R = compute_liang_nvar(xx, dt, n_iter)\n",
    "    print(f\"LKIFR computation time: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7fb98b0-a86e-4312-b16c-8c1f00cf7ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Liang-Kleeman Info Transfer (T):\n",
      "[[-0.97944602  0.00578939]\n",
      " [ 0.10135035 -1.0512221 ]]\n",
      "\n",
      "Normalized Info Transfer (% tau):\n",
      "[[-49.96531961   0.27532049]\n",
      " [  5.17027232 -49.9919793 ]]\n",
      "\n",
      "Pearson Correlation (R):\n",
      "[[1.         0.22942272]\n",
      " [0.22942272 1.        ]]\n",
      "\n",
      "Significance of T (1=significant):\n",
      "[[1. 0.]\n",
      " [1. 1.]]\n",
      "\n",
      "Significance of tau:\n",
      "[[1. 0.]\n",
      " [1. 1.]]\n",
      "\n",
      "Significance of R:\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Significance Check\n",
    "conf = 1.96  # 95% confidence\n",
    "sig_T = np.zeros((nvar, nvar))\n",
    "sig_tau = np.zeros((nvar, nvar))\n",
    "sig_R = np.zeros((nvar, nvar))\n",
    "\n",
    "for j in range(nvar):\n",
    "    for k in range(nvar):\n",
    "        sig_T[j, k] = compute_sig(T[j, k], error_T[j, k], conf)\n",
    "        sig_tau[j, k] = compute_sig(tau[j, k], error_tau[j, k], conf)\n",
    "        sig_R[j, k] = compute_sig(R[j, k], error_R[j, k], conf)\n",
    "\n",
    "# Results\n",
    "print(\"\\nLiang-Kleeman Info Transfer (T):\")\n",
    "print(T)\n",
    "print(\"\\nNormalized Info Transfer (% tau):\")\n",
    "print(tau)\n",
    "print(\"\\nPearson Correlation (R):\")\n",
    "print(R)\n",
    "\n",
    "print(\"\\nSignificance of T (1=significant):\")\n",
    "print(sig_T)\n",
    "print(\"\\nSignificance of tau:\")\n",
    "print(sig_tau)\n",
    "print(\"\\nSignificance of R:\")\n",
    "print(sig_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849f62b-70fa-418e-afa0-11fe9868301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_series==False:\n",
    "    # Save time series and results\n",
    "    np.save('../data/2D_series_new.npy', [t, X1, X2])\n",
    "    np.save('../data/2D_liang_new.npy', [T, tau, R, error_T, error_tau, error_R, sig_T, sig_tau, sig_R])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcf7bc-5716-48da-aeec-513694cd0142",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCMCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44bb8ad4-97c0-4355-9417-d31cf8ead461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tigramite.data_processing import DataFrame\n",
    "from tigramite.independence_tests.parcorr import ParCorr\n",
    "from tigramite.pcmci import PCMCI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc898c-9302-407f-86b1-8e99aff9e149",
   "metadata": {},
   "source": [
    "Tigramite takes input data as 2D Numpy array of shape (T:num of timesteps, N:num of variables)\n",
    "Our simulated series X1, X2 are shaped (nt,) that is, each is a separate 1D timeseries. So, \n",
    "we will make the array (2, T) and then we\\ll transpose it to be ready for Tigramite's DataFrame class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5119d8-4936-4f78-accd-249713cc7975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCMCI Path Coefficients (val_matrix) at lag=1:\n",
      "[[0.70656187 0.00194443]\n",
      " [0.00071363 0.7062371 ]]\n",
      "\n",
      "Inferred parents per variable (from graph):\n",
      "{0: [(0, -1)], 1: [(1, -1)]}\n",
      "\n",
      "FDR-corrected for significant p-values in q-matrix:\n",
      "[[[False  True False False]\n",
      "  [False False False False]]\n",
      "\n",
      " [[False False False False]\n",
      "  [False  True False False]]]\n"
     ]
    }
   ],
   "source": [
    "data_raw = np.array([X1[start_computation:], X2[start_computation:]]).T  # shape: (T, 2)\n",
    "dataframe = DataFrame(data_raw, var_names=[\"X1\", \"X2\"])\n",
    "\n",
    "# Set up ParCorr test and PCMCI object\n",
    "parcorr = ParCorr(significance='analytic')\n",
    "pcmci = PCMCI(dataframe=dataframe, cond_ind_test=parcorr)\n",
    "\n",
    "# Run PCMCI (includes graph output by thresholding p_matrix)\n",
    "results = pcmci.run_pcmci(tau_max=3, pc_alpha=0.05)\n",
    "\n",
    "# FDR-corrected p-values\n",
    "q_matrix = pcmci.get_corrected_pvalues(p_matrix=results['p_matrix'], fdr_method='fdr_bh')\n",
    "alpha_level = 0.05\n",
    "significant = q_matrix < alpha_level\n",
    "\n",
    "# Retrieve inferred parents from graph\n",
    "parents_dict = pcmci.return_parents_dict(graph=results['graph'], val_matrix=results['val_matrix'])\n",
    "\n",
    "# Show beta matrix at lag 1\n",
    "print(\"\\nPCMCI Path Coefficients (val_matrix) at lag=1:\")\n",
    "print(results['val_matrix'][:, :, 1])\n",
    "\n",
    "print(\"\\nInferred parents per variable (from graph):\")\n",
    "print(parents_dict)\n",
    "\n",
    "print(\"\\nFDR-corrected for significant p-values in q-matrix:\")\n",
    "print(significant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80448faf-d51e-4bda-8071-1f2d4a74e406",
   "metadata": {},
   "source": [
    "# 6D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "839ddcea-9ad2-40b1-a5d5-8b38c58e846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_comp.function_liang_nvar import compute_liang_nvar, compute_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec356a-6847-44e4-9a77-429ed3e2bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "save_var = False # True: save Liang index and correlation coefficient (for plotting afterwards); False: don't save variables\n",
    "load_series = True # True: load already saved time series (x,y,z); False: compute and save time series\n",
    "nvar = 6 # number of variables (Liang [2021]: nvar=6)\n",
    "n_iter = 400 # nuof bootstrap realizations (for computing the error in Liang index)\n",
    "b_factor = 1 # amplitude of stochastic perturbation\n",
    "conf = 1.96 # 1.96 if 95% confidence interval; 2.58 if 99% confidence interval; 1.65 if 90% confidence interval\n",
    "\n",
    "# Time parameters\n",
    "dt = 1 # time step\n",
    "tmax = 1.e6 # total duration in unit times\n",
    "nt = int(tmax / dt) # number of time steps (Liang [2021]: nt=10000)\n",
    "t = np.linspace(0,tmax,nt) # time vector (varying between 0 and tmax with nt time steps)\n",
    "start_computation = int(10000 / dt) # exclude the first transient times for computing correlation coefficient and rate of information transfer\n",
    "\n",
    "# VAR parameters\n",
    "alpha = np.array([0.1,0.7,0.5,0.2,0.8,0.3])\n",
    "A = np.array([(0, 0, -0.6, 0, 0, 0),\n",
    "\t\t\t  (-0.5, 0, 0, 0, 0, 0.8),\n",
    "\t\t\t  (0, 0.7, 0, 0, 0, 0),\n",
    "\t\t\t  (0, 0, 0, 0.7, 0.4, 0),\n",
    "\t\t\t  (0, 0, 0, 0.2, 0, 0.7),\n",
    "\t\t\t  (0, 0, 0, 0, 0, -0.5)])\n",
    "B = np.ones(nvar) * b_factor\n",
    "\n",
    "# Random errors\n",
    "mean_e = 0\n",
    "std_e = 1\n",
    "e = np.zeros((nvar,nt))\n",
    "for var in np.arange(nvar):\n",
    "    e[var,:] = np.random.normal(mean_e,std_e,nt)\n",
    "\n",
    "# Initialization of variables\n",
    "X = np.zeros((nvar,nt))\n",
    "T = np.zeros((nvar,nvar))\n",
    "tau = np.zeros((nvar,nvar))\n",
    "R = np.zeros((nvar,nvar))\n",
    "error_T = np.zeros((nvar,nvar))\n",
    "error_tau = np.zeros((nvar,nvar))\n",
    "error_R = np.zeros((nvar,nvar))\n",
    "\n",
    "# Time series\n",
    "filename = '../data/6D_series.npy'\n",
    "if load_series == True: # load time series\n",
    "    t,X[0,:],X[1,:],X[2,:],X[3,:],X[4,:],X[5,:] = np.load(filename,allow_pickle=True)\n",
    "\n",
    "else: # compute time series and save them\n",
    "\n",
    "    # VAR model\n",
    "    for i in np.arange(nt-1):\n",
    "        for var in np.arange(nvar):\n",
    "            X[var,i+1] = alpha[var] + np.sum(A[var,:] * X[:,i]) + B[var] * e[var,i+1]\n",
    "\n",
    "    # Save time series\n",
    "    np.save(filename,[t, X[0,:], X[1,:], X[2,:], X[3,:], X[4,:], X[5,:]])\n",
    "\n",
    "# Compute rate of information transfer and correlation coefficient using function_liang\n",
    "xx = np.array((X[0,start_computation::],\n",
    "               X[1,start_computation::],\n",
    "               X[2,start_computation::],\n",
    "               X[3,start_computation::],\n",
    "               X[4,start_computation::],\n",
    "               X[5,start_computation::]))\n",
    "T,tau,R,error_T,error_tau,error_R = compute_liang_nvar(xx,dt,n_iter)\n",
    "\n",
    "# Compute significance of rate of information transfer and correlation coefficient (by combining bootstrap samples)\n",
    "sig_T = np.zeros((nvar,nvar))\n",
    "sig_tau = np.zeros((nvar,nvar))\n",
    "sig_R = np.zeros((nvar,nvar))\n",
    "for j in np.arange(nvar):\n",
    "    for k in np.arange(nvar):\n",
    "        sig_T[j,k] = compute_sig(T[j,k],error_T[j,k],conf)\n",
    "        sig_tau[j,k] = compute_sig(tau[j,k],error_tau[j,k],conf)\n",
    "        sig_R[j,k] = compute_sig(R[j,k],error_R[j,k],conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086739d3-8ae8-4763-89d9-99971b5a2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if save_var == True:\n",
    "    filename = '../data/6D_liang.npy'\n",
    "    np.save(filename,[T,tau,R,error_T,error_tau,error_R,sig_T,sig_tau,sig_R])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74d0e315-416e-4207-b7f2-6c69d7d877d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Liang-Kleeman Info Transfer (T) [6D]:\n",
      "[[-9.99813539e-01  1.22778826e-02  9.45702946e-06 -1.99191622e-07\n",
      "   1.47526177e-05  2.13753791e-05]\n",
      " [-1.21380506e-06 -1.00221419e+00 -9.15105787e-02 -1.20269742e-05\n",
      "   5.23070109e-04 -7.74923041e-05]\n",
      " [-3.88334326e-02 -6.16887429e-05 -9.99346085e-01 -3.59360638e-05\n",
      "   7.44331306e-05 -1.05505787e-04]\n",
      " [-9.43335953e-08  2.71796218e-05  3.12757633e-05 -2.99194938e-01\n",
      "   3.70673050e-02 -3.99155694e-05]\n",
      " [-4.70135792e-05  3.91032358e-04  2.54368927e-04  4.57433271e-02\n",
      "  -1.00229140e+00  1.67191010e-05]\n",
      " [-2.82393700e-05 -1.84482243e-01 -7.70080075e-05 -1.70598690e-05\n",
      "  -1.85379191e-01 -1.49975035e+00]]\n",
      "\n",
      "Normalized Info Transfer (% tau) [6D]:\n",
      "[[-7.55675946e+01  8.62996499e-01  7.11341667e-04 -3.84588206e-05\n",
      "   9.80053335e-04  1.13999479e-03]\n",
      " [-9.17414346e-05 -7.04443398e+01 -6.88327006e+00 -2.32210189e-03\n",
      "   3.47488572e-02 -4.13283072e-03]\n",
      " [-2.93509638e+00 -4.33602200e-03 -7.51691124e+01 -6.93833713e-03\n",
      "   4.94477926e-03 -5.62684981e-03]\n",
      " [-7.12989233e-06  1.91042049e-03  2.35250971e-03 -5.77669096e+01\n",
      "   2.46247389e+00 -2.12878289e-03]\n",
      " [-3.55336566e-03  2.74851590e-02  1.91331980e-02  8.83186949e+00\n",
      "  -6.65847275e+01  8.91665502e-04]\n",
      " [-2.13437925e-03 -1.29670184e+01 -5.79241132e-03 -3.29382548e-03\n",
      "  -1.23152039e+01 -7.99849015e+01]]\n",
      "\n",
      "Pearson Correlation (R) [6D]:\n",
      "[[ 1.00000000e+00 -2.77952436e-02  5.91482581e-02 -8.36284943e-04\n",
      "  -5.38980737e-02  3.72576276e-02]\n",
      " [-2.77952436e-02  1.00000000e+00 -1.26374160e-01 -3.71774386e-02\n",
      "   3.66632437e-01 -2.99982001e-01]\n",
      " [ 5.91482581e-02 -1.26374160e-01  1.00000000e+00  6.50224867e-02\n",
      "  -1.37390643e-01  1.09580583e-01]\n",
      " [-8.36284943e-04 -3.71774386e-02  6.50224867e-02  1.00000000e+00\n",
      "   1.45542249e-01  3.49171153e-02]\n",
      " [-5.38980737e-02  3.66632437e-01 -1.37390643e-01  1.45542249e-01\n",
      "   1.00000000e+00 -3.07058889e-01]\n",
      " [ 3.72576276e-02 -2.99982001e-01  1.09580583e-01  3.49171153e-02\n",
      "  -3.07058889e-01  1.00000000e+00]]\n",
      "\n",
      "Significance of T [6D]:\n",
      "[[1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 1.]]\n",
      "\n",
      "Significance of tau [6D]:\n",
      "[[1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 1.]]\n",
      "\n",
      "Significance of R [6D]:\n",
      "[[1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Display\n",
    "print(\"\\nLiang-Kleeman Info Transfer (T) [6D]:\")\n",
    "print(T)\n",
    "print(\"\\nNormalized Info Transfer (% tau) [6D]:\")\n",
    "print(tau)\n",
    "print(\"\\nPearson Correlation (R) [6D]:\")\n",
    "print(R)\n",
    "\n",
    "print(\"\\nSignificance of T [6D]:\")\n",
    "print(sig_T)\n",
    "print(\"\\nSignificance of tau [6D]:\")\n",
    "print(sig_tau)\n",
    "print(\"\\nSignificance of R [6D]:\")\n",
    "print(sig_R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ef848-556f-41db-9c74-bf1dcaf228cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
